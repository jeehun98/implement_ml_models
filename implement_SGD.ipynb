{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMqeAGbI7pJD",
        "outputId": "3900c1d6-d7dd-412d-e494-cac3133d6a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = mnist[\"data\"], mnist[\"target\"]\n",
        "\n",
        "x_train, x_test, y_train, y_test = x[:500], x[60000:], y[:500], y[60000:]\n",
        "\n",
        "y_train_5 = (y_train == 5)\n",
        "y_test_5 = (y_test == 5)"
      ],
      "metadata": {
        "id": "aPOrOGy-8F5_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train)"
      ],
      "metadata": {
        "id": "HmZjFrs78dTa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_5 = np.array(y_train_5).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "RRtWRZu9Xmkr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 활성화 함수\n",
        "class activation_function:\n",
        "  # 시그모이드 함수\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1+np.e**-x)\n",
        "  \n",
        "  # 시그모이드 함수의 미분 함수\n",
        "  def sigmoid_diff(self, x):  \n",
        "    return self.sigmoid(x) * (1 - self.sigmoid(x))"
      ],
      "metadata": {
        "id": "VEwhmZxJfoVo"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 비용 함수 클래스, 비용 함수의 종류에 따라 다른 클래스 내 함수를 사용한다.\n",
        "class cost_function:\n",
        "  # 예측값\n",
        "  predict = []\n",
        "  # 타겟값\n",
        "  target = []\n",
        "  # 비용 함수값\n",
        "  error_cost = []\n",
        "\n",
        "  # 오차 제곱합\n",
        "  def errer_squared_sum(self, predict, target):\n",
        "    self.predict = predict\n",
        "    self.target = target\n",
        "\n",
        "    self.error_cost = np.sum(0.5*((predict - target)**2))\n",
        "    return self.error_cost\n",
        "\n",
        "  # 오차 제곱합 미분 함수\n",
        "  def diff_error_squared_sum(self, predict, target):\n",
        "    return predict - target"
      ],
      "metadata": {
        "id": "G-3vb4DxpEtN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class gradient_descent:\n",
        "  # 각 값에 대한 가중치\n",
        "  weight = []\n",
        "\n",
        "  # 활성화 함수\n",
        "  activation = activation_function()\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = cost_function()\n",
        "\n",
        "  # 델타 값\n",
        "  delta = []\n",
        "\n",
        "  def gradient_descent(self, train, target, learning_rate, iterations):\n",
        "    # 입력값의 변화, 편향값을 계산해주기 위한 1의 추가\n",
        "    train_b = np.c_[train, np.ones((train.shape[0], 1))]\n",
        "\n",
        "    # 임의의 가중치 생성\n",
        "    self.weight = np.random.rand(train.shape[1] + 1, 1)\n",
        "\n",
        "    # 가중치와의 합성곱 연산\n",
        "    predict = train_b @ self.weight\n",
        "\n",
        "    predict = self.activation.sigmoid(predict)\n",
        "\n",
        "    self.cal_delta(predict, target)\n",
        "\n",
        "    self.weight_update(train_b, learning_rate)\n",
        "\n",
        "    self.gradient_descent_iterations(train_b, target, iterations, learning_rate)\n",
        "\n",
        "  # 가중치의 변화량에 대한 비용함수의 변화량을 구하기 위한 cal_delta 함수\n",
        "  def cal_delta(self, predict, target):\n",
        "    delta = self.cost.diff_error_squared_sum(predict, target)\n",
        "\n",
        "    self.delta = delta\n",
        "\n",
        "  # 가중치 업데이트 함수\n",
        "  def weight_update(self, train, learning_rate):\n",
        "    weight_update = self.delta * train\n",
        "    weight_update = weight_update.mean(axis=0).reshape(-1, 1)\n",
        "\n",
        "    self.weight = self.weight - weight_update * learning_rate\n",
        "\n",
        "    print(weight_update)\n",
        "\n",
        "  # 반복 학습을 위한 함수\n",
        "  def gradient_descent_iterations(self, train, target, iterations, learning_rate):\n",
        "  \n",
        "    for i in range(iterations):\n",
        "      # 가중치와의 합성곱 연산\n",
        "      predict = train @ self.weight\n",
        "\n",
        "      predict = self.activation.sigmoid(predict)\n",
        "\n",
        "      self.cal_delta(predict, target)\n",
        "\n",
        "      self.weight_update(train, learning_rate)"
      ],
      "metadata": {
        "id": "kuFt10qdBS8T"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gd = gradient_descent()"
      ],
      "metadata": {
        "id": "pM9VgS3RPsqj"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gd.gradient_descent(x_train, y_train_5, 0.001, 10)"
      ],
      "metadata": {
        "id": "BrLUlt_9XjCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rl45hLJ5e9Ii"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}