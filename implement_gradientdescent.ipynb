{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4aMQyLjt_6C"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "\n",
        "  w = []\n",
        "\n",
        "  def gradientDescent(self, input, target, iterations, learing_rate):\n",
        "    \"\"\"\n",
        "    경사 하강법\n",
        "    Args\n",
        "      input : 입력 데이터\n",
        "      target : 타겟 데이터\n",
        "      iterations : 반복 횟수\n",
        "      learning_rate : 학습률\n",
        "    \"\"\"\n",
        "    # 입력 데이터의 마지막 열의 원소 1의 추가\n",
        "    input_b = np.c_[input, np.ones((input.shape[0], 1))]\n",
        "\n",
        "    # 임의의 가중치 생성\n",
        "    w = np.random.randn(input.shape[1] + 1, 1)\n",
        "\n",
        "    for i in range(iterations):\n",
        "      gradients = 2 / m * input_b.T.dot(input_b.dot(w) - target)\n",
        "      w = theta - learning_rate * gradients\n",
        "\n",
        "    self.w = w\n",
        "\n",
        "  def stochasticGradientDescent(self, input, target, iterations, learning_rate):\n",
        "    \"\"\"\n",
        "    확률적 경사 하강법법\n",
        "    Args\n",
        "      input : 입력 데이터\n",
        "      target : 타겟 데이터\n",
        "      iterations : 반복 횟수\n",
        "      learning_rate : 학습률\n",
        "    \"\"\"\n",
        "    # 입력 데이터의 마지막 열의 원소 1의 추가\n",
        "    input_b = np.c_[input, np.ones((input.shape[0], 1))]\n",
        "\n",
        "    # 임의의 가중치 생성\n",
        "    w = np.random.randn(input.shape[1] + 1, 1)\n",
        "\n",
        "    for i in range(iterations):\n",
        "      for j in range(input_b.shape[0]):\n",
        "        random_index = np.random.randint(input_b.shape[0]):\n",
        "        input_i = input_b[random_index + 1]\n",
        "        target_i = target[random_index + 1]\n",
        "        gradients = 2 / m * input_i.T.dot(input_i.dot(w) - target_i)\n",
        "        \n",
        "        # 학습의 반복 횟수가 증가할수록 학습률이 감소\n",
        "        eta = 5 / (i*input_b.shape[0] + i + 50)\n",
        "        w = theta - eta * gardients\n",
        "    \n",
        "    self.w = w\n",
        "\n",
        "  \n",
        "  def miniBatchGrdientDescent(self, input, target, iterations, learning_rate, batch_count):\n",
        "    \"\"\"\n",
        "    미니배치 경사 하강법\n",
        "    Args\n",
        "      input : 입력 데이터\n",
        "      target : 타겟 데이터\n",
        "      iterations : 반복 횟수\n",
        "      learning_rate : 학습률\n",
        "      batch_count : 배치의 개수\n",
        "    \"\"\"\n",
        "    # 입력 데이터가 섞여있다고 가정, 순서대로 데이터를 분류한다.\n",
        "\n",
        "    #  각 배치 별 데이터 개수\n",
        "    batch_size = int(input_b.shpae[0] / batch_count)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      input_m = input[i*batch_size:(i+1) * batch_size]\n",
        "      self.gradientDescent(input, target, iterations, laerning_rate)\n",
        "    \n",
        "    # 남는 데이터에 대한 연산\n",
        "    input_m = input[batch_size * batch_count:]\n",
        "    self.gradientDescent(input, target, iterations, laerning_rate)\n",
        "\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "7KrVcXp6uG_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}